# Argus: Resilience-Oriented Safety Assurance Framework for End-to-End ADSs

In this work, we propose a runtime resilience-oriented framework, **Argus**, to mitigate the driving hazards, thus preventing potential safety violations and improving the driving performance of an ADS. Argus continuously monitors the trajectories generated by the ADS for potential hazards and, whenever the EGO vehicle is deemed unsafe, seamlessly takes control through a hazard mitigator. 

![Overview](./assets/Overview.png)

Note: This is the first version of Argus, and we plan to release updates in the future.

This work has been submitted to ASE 2025.

## Benchmark

### Bench2Drive

[Bench2Drive](https://github.com/Thinklab-SJTU/Bench2Drive) is a CARLA benchmark proposed by the paper [Bench2Drive: Towards Multi-Ability Benchmarking of Closed-Loop End-To-End Autonomous Driving](https://arxiv.org/abs/2406.03877). It consists of 220 very short (~150m) routes split across all towns with 1 safety critical scenario in each route. The routes are stored in [there](leaderboard/data/bench2drive220.xml). 

### CARLA Leaderboard 2.0 validation set

[The CARLA leaderboard 2.0 validation set](https://leaderboard.carla.org/) is a set of 20 long (~12 km) routes in Town 13. While driving along the routes, the agent has to solve around 90 safety critical scenarios per route consisting of 21 different types (38 counting variations). Due to the length of the routes as well as the large number of scenarios per route, the scores on this benchmark are much lower than on Bench2Drive and other benchmarks. The routes are stored in [there](leaderboard/data/val20.xml).



## Target ADSs and Checkpoints

We choose three state-of-the-art models, which are the most representative end-to-end ADSs, as our targets. All the three ADSs are initially trained with the Bench2Drive training set:

* [UniAD](https://github.com/OpenDriveLab/UniAD)
* [VAD](https://github.com/hustvl/VAD)
* [TCP](https://github.com/OpenDriveLab/TCP)

We adopt a widely used approach, BEVFormer, which aggregates multiview camera inputs to construct the surrounding environment of the ADS. 

* [BEVFormer](https://github.com/fundamentalvision/BEVFormer)

The checkpoints of the ADS models and BEV perception models are available [here](https://drive.google.com/drive/folders/1Hn-81QSRjcxE1uKiL9QBDRVgGEcp9uKI?usp=drive_link).
Please refer to the [website](https://argus4ads.github.io/Argus/) and paper for details.

## Experiment Environment

We recommend and conduct all the experiments on a system with the following specifications:

* Operating System: Ubuntu 20.04.4 LTS
* GPU: 4 NVIDIA GeForce RTX 3090 GPUs
* CPU: Intel(R) Xeon(R) Silver 4310 @ 2.10GHz 
* Memory: 128GB

In this configuration, it takes at least 5 days to complete an evaluation of 240 routes for two Benchmarks (Bench2Drive and CARLA leaderboard 2.0 validation set).

## Installation

Follow the steps below to install Argus:

* **Step 1: Clone the repository:**
    ```bash
    git clone https://github.com/Argus4ADS/Argus.git
    ```
* **Step 2: Create environment:**
    ```bash
    ## python3.8 is recommended
    conda create -n argus python=3.8
    conda activate argus
    ```
* **Step 3: Install torch:**
    ```bash
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
    ```
* **Step 4: Set environment variables:**
    ```bash
    # cuda 11.8 and GCC 9.4 is strongly recommended. Otherwise, it might encounter errors.
    cd Bench2DriveZoo
    export PATH=YOUR_GCC_PATH/bin:$PATH
    export CUDA_HOME=YOUR_CUDA_PATH/
    ```
* **Step 5: Install ninja and packaging:**
    ```bash
    pip install ninja packaging
    ```
* **Step 6: Install mmcv:**
    ```bash
    pip install -v -e .
    ```
* **Step 7: Prepare pretrained weights:**
    ```bash
    # create directory ckpts
    mkdir ckpts 
    ```
    Download the pretrained weights from [here](https://drive.google.com/drive/folders/1Hn-81QSRjcxE1uKiL9QBDRVgGEcp9uKI?usp=drive_link) and put all `*.pth` in the `ckpts` directory.
* **Step 8: Install additional package:**
    ```bash
    pip install -r requirements_add.txt
    ```
* **Step 9: Install CARLA for closed-loop evaluation.:**
    ```bash
    ## Ignore the line about downloading and extracting CARLA if you have already done so.
    mkdir carla
    cd carla
    wget https://carla-releases.s3.us-east-005.backblazeb2.com/Linux/CARLA_0.9.15.tar.gz
    tar -xvf CARLA_0.9.15.tar.gz
    cd Import && wget https://carla-releases.s3.us-east-005.backblazeb2.com/Linux/AdditionalMaps_0.9.15.tar.gz
    cd .. && bash ImportAssets.sh
    export CARLA_ROOT=YOUR_CARLA_PATH

    ## Important!!! Otherwise, the python environment can not find carla package
    echo "$CARLA_ROOT/PythonAPI/carla/dist/carla-0.9.15-py3.7-linux-x86_64.egg" >> YOUR_CONDA_PATH/envs/YOUR_CONDA_ENV_NAME/lib/python3.8/site-packages/carla.pth # python 3.8 works well, please set YOUR_CONDA_PATH and YOUR_CONDA_ENV_NAME
    ```

## Evaluation

* Add your agent to leaderboard/team_code/your_agent.py & Link your model folder under the Bench2Drive directory.
  
  ```bash
      Arugus\ 
      assets\
      leaderboard\
        team_code\
          --> Please add your agent HEAR
      scenario_runner\
      tools\
      Bench2DriveZoo\
    ```
    We recommend you to use link:
    ```bash
    # Add your agent code
    cd leaderboard
    mkdir team_code
    ln -s Bench2DriveZoo/team_code/* ./team_code    # agents and utils 
    ```
* Run the evaluation script:
    
    Multi-Process Multi-GPU Parallel Eval. If your team_agent saves any image for debugging, it might take lots of disk space.
    * For Baseline：
        ```bash
        # Please set TASK_NUM, GPU_RANK_LIST, TASK_LIST, TEAM_AGENT, TEAM_CONFIG, recommend GPU: Task(1:2).
        # It is normal that certain model can not finsih certain routes, no matter how many times we restart the evaluation. It should be treated as failing as it usually happens in the routes where agents behave badly.
        bash leaderboard/scripts/run_evaluation_multi_uniad.sh
        ```
    * For Argus：
        ```bash
        # Please set TASK_NUM, GPU_RANK_LIST, TASK_LIST, TEAM_AGENT, TEAM_CONFIG, recommend GPU: Task(1:2).
        # It is normal that certain model can not finsih certain routes, no matter how many times we restart the evaluation. It should be treated as failing as it usually happens in the routes where agents behave badly.
        bash leaderboard/scripts/run_evaluation_multi_uniad_argus.sh
        ```

* Visualization - make a video for debugging with canbus info printed on the sequential images.
    ```
    python tools/generate_video.py -f your_rgb_folder/
    ```

* Metric:
  * For **Bench2Drive220**, Make sure there are exactly 220 routes in your json. Failed/Crashed status is also acceptable. Otherwise, the metric is inaccurate.
    ```
    # Merge eval json and get driving score and success rate
    # This script will assume the total number of routes with results is 220. If there is not enough, the missed ones will be treated as 0 score.
    python tools/merge_route_json.py -f your_json_folder/

    # Get multi-ability results
    python tools/ability_benchmark.py -r merge.json
    ```
  * For CVL20, you need to run scripts to correct DS, flowing [here](https://leaderboard.carla.org/evaluation_v2_1/#:~:text=global%20infraction%20penalty.-,Infractions,on%20their%20severity%20and%20are%20aggregatted%20into%20the%20infraction%20penalty%20formula%3A,-P).
    ```bash
    python tools/merge_route_json_val20.py -f your_json_folder/
    # Correct the DS
    python tools/merge_route_correct4val20.py
    ```

## Deal with CARLA

This part comes from Bench2Drive, but it's important to deal with some bugs caused by Carla. Thanks to the author of Bench2Drive.

- CARLA has complex dependencies and is not stable. Please check the issue section of CARLA **very carefully**.
- Use tools/clean_carla.sh frequently and multiple times. Some CARLA processes are difficult to kill. Be sure to clean_carla could avoid lots of bugs.
- In our evaluation tools, the launch of CARLA is automatic: https://github.com/Thinklab-SJTU/Bench2Drive/tree/main/leaderboard/leaderboard/leaderboard_evaluator.py#L203. But you could always start CARLA by the one single command line to debug.
- CARLA is not controlled CUDA_VISIBLE_DEVICES! It is controlled by -graphicsadapter in the command line. **Interestingly, in some machines, for some unknown reasons, -graphicsadapter=1 is not available.** For example, with 4 GPUS, it might be: GPU0 -graphicsadapter=0, GPU1  -graphicsadapter=2, GPU2 -graphicsadapter=3, GPU3  -graphicsadapter=4.
- The conflict of PORT is frequently happened. Use lsof-i:YOUR_PORT frequently to avoid conflict. Avoid use small port numbers (<10000 could be unsafe).
- *4.26.2-0+++UE4+Release-4.26 522 0 Disabling core dumps*. Only showing these two lines without termination is good. *WARNING: lavapipe is not a conformant vulkan implementation, testing use only.* is bad.
- **If you face issues, always try to start CARLA in one single line to make sure CARLA could run.** If CARLA is finished immediately, it is very possible to be related to Vulkan. *Try /usr/bin/vulkaninfo | head -n 5*
- Re-install vulkan might be helpful *sudo apt install vulkan-tools; sudo apt install vulkan-utils*  In the end, you need to make sure your vulkan is correct. We have tested *Vulkan Instance Version: 1.x WARNING: lavapipe is not a conformant vulkan implementation, testing use only.* and version 1.1/1.2/1.3 works fine.
- We find that nvidia driver version 470 is good all the time. 515 has some problems but okay. 550 has lots of bugs.
- *sleep* is important to avoid crash of CARLA. For example, https://github.com/Thinklab-SJTU/Bench2Drive/blob/main/leaderboard/leaderboard/leaderboard_evaluator.py#L207, the sleep time should be extended for slower machines. When it comes to multi-gpu evaluation, https://github.com/Thinklab-SJTU/Bench2Drive/blob/main/leaderboard/scripts/run_evaluation_multi_uniad.sh#L58, the sleep time should also be extended for slower machines.

## Acknowledgements

Our code is build on the shoulders of many other open source repositories.
Particularly, we would like to thank the following repositories for their contributions:
* [scenario_runner](https://github.com/carla-simulator/scenario_runner)
* [Bench2Drive](https://github.com/Thinklab-SJTU/Bench2Drive)
* [leaderboard](https://github.com/carla-simulator/leaderboard)
* [carla_garage](https://github.com/autonomousvision/carla_garage)
* [BEVFormer](https://github.com/fundamentalvision/BEVFormer)
* [UniAD](https://github.com/OpenDriveLab/UniAD)
* [VAD](https://github.com/hustvl/VAD)
* [TCP](https://github.com/OpenDriveLab/TCP)
* [DriveLM](https://github.com/OpenDriveLab/DriveLM)

We also thank the creators of the numerous pip libraries we use. Complex projects like this would not be feasible without your contribution.